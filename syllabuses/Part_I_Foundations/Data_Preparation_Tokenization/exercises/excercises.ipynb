{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a3e5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n- 'stopwords': A corpus of common words (e.g., 'and', 'the') that are often filtered out in text processing.\\n- 'PorterStemmer': A stemming algorithm to reduce words to their root form.\\n- 'string': A module that provides string constants and functions.\\n\\nAdditionally, download essential NLTK resources:\\n- 'stopwords': To access the list of common stop words.\\n- 'punkt': To enable sentence and word tokenization.\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\"\"\"\n",
    "Import the 'sent_tokenize' and 'word_tokenize' functions from the NLTK tokenizer module.\n",
    "'sent_tokenize' is used to split a given text into individual sentences,\n",
    "while 'word_tokenize' is used to break the text down into individual words.\n",
    "These functions are essential for preprocessing text in natural language processing tasks.\n",
    "\"\"\"\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\"\"\"\n",
    "Download the 'punkt' tokenizer models from the NLTK data repository.\n",
    "This package provides pre-trained models for tokenizing text into sentences and words,\n",
    "which are essential for various natural language processing tasks.\n",
    "Ensure that the models are available for use in text analysis, sentiment analysis, and more.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\"\"\"\n",
    "- 'stopwords': A corpus of common words (e.g., 'and', 'the') that are often filtered out in text processing.\n",
    "- 'PorterStemmer': A stemming algorithm to reduce words to their root form.\n",
    "- 'string': A module that provides string constants and functions.\n",
    "\n",
    "Additionally, download essential NLTK resources:\n",
    "- 'stopwords': To access the list of common stop words.\n",
    "- 'punkt': To enable sentence and word tokenization.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b24cb7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#d8b4fe;; padding:15px; border-radius:10px;\">\n",
    "\n",
    "## ðŸ“˜ Exercise 1 â€” Corpus Loading & Tokenization\n",
    "**Task:**  \n",
    "1. Load the provided corpus file: `exercise1_corpus.txt`.  \n",
    "2. Tokenize the text into words using **NLTK**.  \n",
    "3. Count the frequency of each token.  \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ead0a640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senetece corpus 1:\n",
      "['Sunflowers are a type of plant that follow the sun, turning their heads from east to west as the day progresses.', 'They are bright, tall, and full of seeds that humans eat, while birds and insects also rely on them for nourishment.', 'Some cultures even associate sunflowers with loyalty, longevity, and adoration, making them a symbol of positivity.', 'Roses, with their delicate petals and enchanting fragrance, have long been cultivated for ornamental and ceremonial purposes.', 'Their colors, ranging from deep crimson to soft pastels, carry various symbolic meanings such as love, friendship, and remembrance.', 'Tulips emerge in early spring, creating vibrant carpets of red, yellow, purple, and white.', 'Originating in Central Asia, tulips became an iconic flower in Dutch culture, sparking the historical phenomenon known as \"tulip mania.\"', 'Bamboo is a fast-growing grass that can reach impressive heights in a short period.', 'It is highly valued for its versatility, being used in construction, furniture, paper production, and even as a food source in some Asian cuisines.', 'Cacti have adapted to arid environments with thick, water-retentive stems and spines that reduce water loss.', 'Despite their harsh habitats, they bloom with remarkable flowers, often attracting pollinators during brief seasonal rains.', 'Orchids are among the most diverse families of flowering plants, with thousands of species across the globe.', 'They are known for their exotic shapes, intricate patterns, and ecological relationships with pollinators such as bees, birds, and even moths.', 'Lavender, with its soothing aroma and purple flowers, is widely used in aromatherapy, cosmetics, and culinary applications.', 'It thrives in Mediterranean climates and attracts pollinators like bees and butterflies.', 'Maple trees are celebrated for their stunning autumn foliage, with leaves turning shades of red, orange, and gold.', 'Their sap can be collected to produce maple syrup, a natural sweetener treasured in North America.', 'Fern species thrive in shaded, humid environments, often covering forest floors and creating lush green undergrowth.', 'They are among the oldest plant lineages on Earth, with fossil records dating back hundreds of millions of years.', 'Dandelions, commonly found in lawns and meadows, are resilient plants whose seeds disperse through wind.', 'Although often considered weeds, every part of the plant can be used for food, medicine, or natural dyeing.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tokenized corpus 1:\n",
      "['Sunflowers', 'are', 'a', 'type', 'of', 'plant', 'that', 'follow', 'the', 'sun', 'turning', 'their', 'heads', 'from', 'east', 'to', 'west', 'as', 'the', 'day', 'progresses', 'They', 'are', 'bright', 'tall', 'and', 'full', 'of', 'seeds', 'that', 'humans', 'eat', 'while', 'birds', 'and', 'insects', 'also', 'rely', 'on', 'them', 'for', 'nourishment', 'Some', 'cultures', 'even', 'associate', 'sunflowers', 'with', 'loyalty', 'longevity', 'and', 'adoration', 'making', 'them', 'a', 'symbol', 'of', 'positivity', 'Roses', 'with', 'their', 'delicate', 'petals', 'and', 'enchanting', 'fragrance', 'have', 'long', 'been', 'cultivated', 'for', 'ornamental', 'and', 'ceremonial', 'purposes', 'Their', 'colors', 'ranging', 'from', 'deep', 'crimson', 'to', 'soft', 'pastels', 'carry', 'various', 'symbolic', 'meanings', 'such', 'as', 'love', 'friendship', 'and', 'remembrance', 'Tulips', 'emerge', 'in', 'early', 'spring', 'creating', 'vibrant', 'carpets', 'of', 'red', 'yellow', 'purple', 'and', 'white', 'Originating', 'in', 'Central', 'Asia', 'tulips', 'became', 'an', 'iconic', 'flower', 'in', 'Dutch', 'culture', 'sparking', 'the', 'historical', 'phenomenon', 'known', 'as', '``', 'tulip', 'mania', \"''\", 'Bamboo', 'is', 'a', 'fast-growing', 'grass', 'that', 'can', 'reach', 'impressive', 'heights', 'in', 'a', 'short', 'period', 'It', 'is', 'highly', 'valued', 'for', 'its', 'versatility', 'being', 'used', 'in', 'construction', 'furniture', 'paper', 'production', 'and', 'even', 'as', 'a', 'food', 'source', 'in', 'some', 'Asian', 'cuisines', 'Cacti', 'have', 'adapted', 'to', 'arid', 'environments', 'with', 'thick', 'water-retentive', 'stems', 'and', 'spines', 'that', 'reduce', 'water', 'loss', 'Despite', 'their', 'harsh', 'habitats', 'they', 'bloom', 'with', 'remarkable', 'flowers', 'often', 'attracting', 'pollinators', 'during', 'brief', 'seasonal', 'rains', 'Orchids', 'are', 'among', 'the', 'most', 'diverse', 'families', 'of', 'flowering', 'plants', 'with', 'thousands', 'of', 'species', 'across', 'the', 'globe', 'They', 'are', 'known', 'for', 'their', 'exotic', 'shapes', 'intricate', 'patterns', 'and', 'ecological', 'relationships', 'with', 'pollinators', 'such', 'as', 'bees', 'birds', 'and', 'even', 'moths', 'Lavender', 'with', 'its', 'soothing', 'aroma', 'and', 'purple', 'flowers', 'is', 'widely', 'used', 'in', 'aromatherapy', 'cosmetics', 'and', 'culinary', 'applications', 'It', 'thrives', 'in', 'Mediterranean', 'climates', 'and', 'attracts', 'pollinators', 'like', 'bees', 'and', 'butterflies', 'Maple', 'trees', 'are', 'celebrated', 'for', 'their', 'stunning', 'autumn', 'foliage', 'with', 'leaves', 'turning', 'shades', 'of', 'red', 'orange', 'and', 'gold', 'Their', 'sap', 'can', 'be', 'collected', 'to', 'produce', 'maple', 'syrup', 'a', 'natural', 'sweetener', 'treasured', 'in', 'North', 'America', 'Fern', 'species', 'thrive', 'in', 'shaded', 'humid', 'environments', 'often', 'covering', 'forest', 'floors', 'and', 'creating', 'lush', 'green', 'undergrowth', 'They', 'are', 'among', 'the', 'oldest', 'plant', 'lineages', 'on', 'Earth', 'with', 'fossil', 'records', 'dating', 'back', 'hundreds', 'of', 'millions', 'of', 'years', 'Dandelions', 'commonly', 'found', 'in', 'lawns', 'and', 'meadows', 'are', 'resilient', 'plants', 'whose', 'seeds', 'disperse', 'through', 'wind', 'Although', 'often', 'considered', 'weeds', 'every', 'part', 'of', 'the', 'plant', 'can', 'be', 'used', 'for', 'food', 'medicine', 'or', 'natural', 'dyeing']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "top 5 tokens frequencies corpus 1:\n",
      "[('and', 18), ('in', 11), ('of', 10), ('with', 9), ('are', 7)]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"./excercise_corpora/exercise1_corpus.txt\") as corp:\n",
    "    corpus1 = corp.read() # upload my corpus\n",
    "    \n",
    "sentences1 = sent_tokenize(corpus1) # seperating by sentence \n",
    "tokens1 =[word for word in word_tokenize(corpus1) if word not in string.punctuation] # extracting tokenized words\n",
    "frequencies1 = nltk.FreqDist(tokens1) # getting frequencies \n",
    "\n",
    "print(\"senetece corpus 1:\") \n",
    "print(sentences1)\n",
    "print(\"-\"*100)\n",
    "print(\"tokenized corpus 1:\") \n",
    "print(tokens1)\n",
    "print(\"-\"*100)\n",
    "print(\"top 5 tokens frequencies corpus 1:\") \n",
    "print(frequencies1.most_common(5))\n",
    "print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934a9d8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#d8b4fe;; padding:15px; border-radius:10px;\">\n",
    "\n",
    "### ðŸ“˜ Exercise 2: Stopword Removal and Stemming\n",
    "\n",
    "**Objective:**  \n",
    "Using NLTK, perform the following tasks on a given corpus:\n",
    "1. Tokenize text into words.\n",
    "2. Remove English stopwords.\n",
    "3. Apply stemming to each remaining token.\n",
    "\n",
    "**Instructions:**  \n",
    "- Use the `nltk.corpus.stopwords` for the stopword list.\n",
    "- Use the `PorterStemmer` for stemming.\n",
    "- Display:\n",
    "  - Original tokenized words\n",
    "  - Tokenized words after stopword removal\n",
    "  - Tokenized words after stemming\n",
    "\n",
    "**Write your solution in the cell below.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f988141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized corpus 2:\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'vital', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', 'it', 'combines', 'linguistics', 'computer', 'science', 'and', 'machine', 'learning', 'to', 'process', 'and', 'analyze', 'vast', 'amounts', 'of', 'textual', 'data', 'applications', 'of', 'nlp', 'span', 'numerous', 'domains', 'including', 'chatbots', 'virtual', 'assistants', 'machine', 'translation', 'speech', 'recognition', 'and', 'sentiment', 'analysis', 'advanced', 'nlp', 'models', 'leverage', 'techniques', 'such', 'as', 'tokenization', 'stemming', 'lemmatization', 'and', 'word', 'embeddings', 'to', 'understand', 'context', 'and', 'meaning', 'transformers', 'recurrent', 'neural', 'networks', 'rnns', 'and', 'long', 'memory', 'networks', 'lstms', 'have', 'significantly', 'improved', 'the', 'accuracy', 'of', 'language', 'models', 'recent', 'breakthroughs', 'in', 'deep', 'learning', 'have', 'enabled', 'applications', 'like', 'automatic', 'summarization', 'question', 'answering', 'and', 'conversational', 'ai', 'that', 'can', 'interact', 'with', 'humans', 'naturally', 'researchers', 'also', 'explore', 'challenges', 'like', 'ambiguity', 'resolution', 'tagging', 'named', 'entity', 'recognition', 'coreference', 'resolution', 'and', 'semantic', 'parsing', 'moreover', 'nlp', 'intersects', 'with', 'information', 'retrieval', 'knowledge', 'representation', 'and', 'recommendation', 'systems', 'to', 'build', 'intelligent', 'systems', 'that', 'comprehend', 'user', 'queries', 'and', 'generate', 'meaningful', 'responses', 'the', 'continuous', 'growth', 'of', 'multilingual', 'and', 'corpora', 'has', 'further', 'fueled', 'advancements', 'in', 'nlp', 'enabling', 'models', 'to', 'understand', 'specialized', 'vocabularies', 'in', 'fields', 'such', 'as', 'healthcare', 'finance', 'and', 'legal', 'domains', 'ethical', 'considerations', 'bias', 'mitigation', 'and', 'explainability', 'remain', 'crucial', 'aspects', 'of', 'designing', 'responsible', 'nlp', 'systems', 'overall', 'nlp', 'empowers', 'computers', 'to', 'read', 'understand', 'and', 'generate', 'human', 'language', 'unlocking', 'numerous', 'opportunities', 'across', 'industries']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Filtered (no stopwords) corpus 2:\n",
      "['natural', 'language', 'processing', 'nlp', 'vital', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans', 'using', 'natural', 'language', 'combines', 'linguistics', 'computer', 'science', 'machine', 'learning', 'process', 'analyze', 'vast', 'amounts', 'textual', 'data', 'applications', 'nlp', 'span', 'numerous', 'domains', 'including', 'chatbots', 'virtual', 'assistants', 'machine', 'translation', 'speech', 'recognition', 'sentiment', 'analysis', 'advanced', 'nlp', 'models', 'leverage', 'techniques', 'tokenization', 'stemming', 'lemmatization', 'word', 'embeddings', 'understand', 'context', 'meaning', 'transformers', 'recurrent', 'neural', 'networks', 'rnns', 'long', 'memory', 'networks', 'lstms', 'significantly', 'improved', 'accuracy', 'language', 'models', 'recent', 'breakthroughs', 'deep', 'learning', 'enabled', 'applications', 'like', 'automatic', 'summarization', 'question', 'answering', 'conversational', 'ai', 'interact', 'humans', 'naturally', 'researchers', 'also', 'explore', 'challenges', 'like', 'ambiguity', 'resolution', 'tagging', 'named', 'entity', 'recognition', 'coreference', 'resolution', 'semantic', 'parsing', 'moreover', 'nlp', 'intersects', 'information', 'retrieval', 'knowledge', 'representation', 'recommendation', 'systems', 'build', 'intelligent', 'systems', 'comprehend', 'user', 'queries', 'generate', 'meaningful', 'responses', 'continuous', 'growth', 'multilingual', 'corpora', 'fueled', 'advancements', 'nlp', 'enabling', 'models', 'understand', 'specialized', 'vocabularies', 'fields', 'healthcare', 'finance', 'legal', 'domains', 'ethical', 'considerations', 'bias', 'mitigation', 'explainability', 'remain', 'crucial', 'aspects', 'designing', 'responsible', 'nlp', 'systems', 'overall', 'nlp', 'empowers', 'computers', 'read', 'understand', 'generate', 'human', 'language', 'unlocking', 'numerous', 'opportunities', 'across', 'industries']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Stemmed corpus 2:\n",
      "['natur', 'languag', 'process', 'nlp', 'vital', 'subfield', 'artifici', 'intellig', 'focus', 'interact', 'comput', 'human', 'use', 'natur', 'languag', 'combin', 'linguist', 'comput', 'scienc', 'machin', 'learn', 'process', 'analyz', 'vast', 'amount', 'textual', 'data', 'applic', 'nlp', 'span', 'numer', 'domain', 'includ', 'chatbot', 'virtual', 'assist', 'machin', 'translat', 'speech', 'recognit', 'sentiment', 'analysi', 'advanc', 'nlp', 'model', 'leverag', 'techniqu', 'token', 'stem', 'lemmat', 'word', 'embed', 'understand', 'context', 'mean', 'transform', 'recurr', 'neural', 'network', 'rnn', 'long', 'memori', 'network', 'lstm', 'significantli', 'improv', 'accuraci', 'languag', 'model', 'recent', 'breakthrough', 'deep', 'learn', 'enabl', 'applic', 'like', 'automat', 'summar', 'question', 'answer', 'convers', 'ai', 'interact', 'human', 'natur', 'research', 'also', 'explor', 'challeng', 'like', 'ambigu', 'resolut', 'tag', 'name', 'entiti', 'recognit', 'corefer', 'resolut', 'semant', 'pars', 'moreov', 'nlp', 'intersect', 'inform', 'retriev', 'knowledg', 'represent', 'recommend', 'system', 'build', 'intellig', 'system', 'comprehend', 'user', 'queri', 'gener', 'meaning', 'respons', 'continu', 'growth', 'multilingu', 'corpora', 'fuel', 'advanc', 'nlp', 'enabl', 'model', 'understand', 'special', 'vocabulari', 'field', 'healthcar', 'financ', 'legal', 'domain', 'ethic', 'consider', 'bia', 'mitig', 'explain', 'remain', 'crucial', 'aspect', 'design', 'respons', 'nlp', 'system', 'overal', 'nlp', 'empow', 'comput', 'read', 'understand', 'gener', 'human', 'languag', 'unlock', 'numer', 'opportun', 'across', 'industri']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"./excercise_corpora/exercise2_corpus.txt\") as corp:\n",
    "    corpus2 = corp.read()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens2 = [word for word in word_tokenize(corpus2.lower()) if word.isalnum()]\n",
    "filtered_tokens2 = [w for w in tokens2 if w not in stop_words]\n",
    "stemmed_tokens2 = [stemmer.stem(w) for w in filtered_tokens2]\n",
    "\n",
    "print(\"Tokenized corpus 2:\")\n",
    "print(tokens2)\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(\"Filtered (no stopwords) corpus 2:\")\n",
    "print(filtered_tokens2)\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(\"Stemmed corpus 2:\")\n",
    "print(stemmed_tokens2)\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0066c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#d8b4fe; padding:15px; border-radius:10px;\">\n",
    "    <h3>ðŸ“˜ Exercise 3: Tokenization, Stopword Removal, and Stemming</h3>\n",
    "    <strong>Tasks:</strong>\n",
    "    <ol>\n",
    "        <li>Load the provided corpus from <code>exercise3_corpus.txt</code>.</li>\n",
    "        <li>Tokenize the text into words.</li>\n",
    "        <li>Remove punctuation and stopwords.</li>\n",
    "        <li>Apply stemming using NLTK's <code>PorterStemmer</code>.</li>\n",
    "        <li>Print the tokenized, filtered, and stemmed lists.</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "144b7815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized corpus 3:\n",
      "['artificial', 'intelligence', 'is', 'revolutionizing', 'multiple', 'industries', 'including', 'healthcare', 'finance', 'and', 'transportation', 'machine', 'learning', 'allows', 'systems', 'to', 'improve', 'automatically', 'through', 'experience', 'natural', 'language', 'understanding', 'helps', 'computers', 'interact', 'with', 'humans', 'seamlessly', 'robotics', 'combines', 'ai', 'with', 'physical', 'systems', 'to', 'perform', 'tasks', 'autonomously', 'ethical', 'considerations', 'are', 'crucial', 'for', 'ai', 'deployment', 'especially', 'regarding', 'privacy', 'and', 'fairness']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Filtered (no stopwords) corpus 3:\n",
      "['artificial', 'intelligence', 'revolutionizing', 'multiple', 'industries', 'including', 'healthcare', 'finance', 'transportation', 'machine', 'learning', 'allows', 'systems', 'improve', 'automatically', 'experience', 'natural', 'language', 'understanding', 'helps', 'computers', 'interact', 'humans', 'seamlessly', 'robotics', 'combines', 'ai', 'physical', 'systems', 'perform', 'tasks', 'autonomously', 'ethical', 'considerations', 'crucial', 'ai', 'deployment', 'especially', 'regarding', 'privacy', 'fairness']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Stemmed corpus 3:\n",
      "['artifici', 'intellig', 'revolution', 'multipl', 'industri', 'includ', 'healthcar', 'financ', 'transport', 'machin', 'learn', 'allow', 'system', 'improv', 'automat', 'experi', 'natur', 'languag', 'understand', 'help', 'comput', 'interact', 'human', 'seamlessli', 'robot', 'combin', 'ai', 'physic', 'system', 'perform', 'task', 'autonom', 'ethic', 'consider', 'crucial', 'ai', 'deploy', 'especi', 'regard', 'privaci', 'fair']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"./excercise_corpora/exercise3_corpus.txt\") as corp:\n",
    "    corpus3 = corp.read()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens3 = [word for word in word_tokenize(corpus3.lower()) if word.isalnum()]\n",
    "filtered_tokens3 = [w for w in tokens3 if w not in stop_words]\n",
    "stemmed_tokens3 = [stemmer.stem(w) for w in filtered_tokens3]\n",
    "\n",
    "print(\"Tokenized corpus 3:\")\n",
    "print(tokens3)\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(\"Filtered (no stopwords) corpus 3:\")\n",
    "print(filtered_tokens3)\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(\"Stemmed corpus 3:\")\n",
    "print(stemmed_tokens3)\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86d311",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFF8DC; padding:15px; border-radius:8px; line-height:1.5;\">\n",
    "\n",
    "# NLP Text Preprocessing Concepts (Exercises 1â€“3)\n",
    "\n",
    "In Exercises 1â€“3, we took a deep dive into processing text corpora through several key steps. Each step has its own purpose and plays a crucial role in getting raw text ready for analysis or modeling in NLP. Hereâ€™s a breakdown of what we did and why it matters:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Tokenized Corpus\n",
    "**What It Is:**  \n",
    "A tokenized corpus is simply the text broken down into **tokens**, which are the smallest units we can work with. These can be words, subwords, or even punctuation marks.\n",
    "\n",
    "**For Example:**  \n",
    "Take this original sentence:  \n",
    "Sunflowers are bright and beautiful.\n",
    "\n",
    "After tokenization, it looks like this:  \n",
    "Tokenized:  \n",
    "['Sunflowers', 'are', 'bright', 'and', 'beautiful', '.']\n",
    "\n",
    "**Why It Matters:**  \n",
    "- This process turns raw text into manageable pieces for computation.  \n",
    "- It allows us to perform frequency analysis, create n-gram models, and develop word embeddings.  \n",
    "- Tokenization is fundamental for nearly all NLP tasks, whether itâ€™s classification, generation, or retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Filtered Corpus (Stopwords Removal & Punctuation Handling)\n",
    "**What It Is:**  \n",
    "A filtered corpus is what you get when we remove **stopwords** (those common, less meaningful words like `the`, `and`, `is`) and punctuation, leaving us with the **content words** that really matter.\n",
    "\n",
    "**For Example:**  \n",
    "Starting with this tokenized list:  \n",
    "['Sunflowers', 'are', 'bright', 'and', 'beautiful']\n",
    "\n",
    "After filtering, we get:  \n",
    "Filtered:  \n",
    "['Sunflowers', 'bright', 'beautiful']\n",
    "\n",
    "**Why It Matters:**  \n",
    "- This step helps reduce noise and dimensionality in our data.  \n",
    "- It allows us to focus on words that carry real semantic meaning.  \n",
    "- Ultimately, it boosts efficiency and performance in tasks like text classification and topic modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Stemmed Corpus\n",
    "**What It Is:**  \n",
    "A stemmed corpus takes each word and reduces it to its **root form** (or stem), often using algorithms like the **PorterStemmer**.\n",
    "\n",
    "**For Example:**  \n",
    "Consider these original words:  \n",
    "running, runs, runner, easily, faster\n",
    "\n",
    "After stemming, we get:  \n",
    "Stemmed:  \n",
    "run, run, runner, easili, faster\n",
    "\n",
    "**Why It Matters:**  \n",
    "- This groups words that are morphologically related under the same stem.  \n",
    "- It helps reduce the vocabulary size and sparsity.  \n",
    "- By doing this, models can recognize that different forms of a word share similar meanings.  \n",
    "- This is particularly useful for information retrieval, search engines, and classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Frequency Analysis\n",
    "**What It Is:**  \n",
    "Frequency analysis involves counting how often each token or stem appears in the corpus to get **word frequencies**.\n",
    "\n",
    "**For Example:**  \n",
    "{'sunflowers': 5, 'bright': 3, 'beautiful': 3}\n",
    "\n",
    "**Why It Matters:**  \n",
    "- This helps us identify the most common words or stems in our text.  \n",
    "- It supports tasks like keyword extraction, weighting in TF-IDF, and feature selection for models.  \n",
    "- It also guides our strategies for subword or BPE tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Overall Goal\n",
    "The entire process of **tokenization â†’ filtering â†’ stemming â†’ frequency analysis** allows us to:  \n",
    "\n",
    "1. Clean and normalize the raw text.  \n",
    "2. Reduce vocabulary size and eliminate noise.  \n",
    "3. Focus on the semantically meaningful units.  \n",
    "4. Prepare our data for various NLP tasks, such as:  \n",
    "   - Text classification  \n",
    "   - Sentiment analysis  \n",
    "   - Named entity recognition  \n",
    "   - Language modeling  \n",
    "\n",
    "**Key Takeaway:**  \n",
    "These preprocessing steps help us create a **structured representation of raw text**, enabling NLP algorithms to learn patterns efficiently and accurately while managing morphological variations and irrelevant content.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371e3b0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
