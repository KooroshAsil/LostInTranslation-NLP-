{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07962ba0",
   "metadata": {},
   "source": [
    "# task 1 \n",
    "get the unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f8a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "    \n",
    "    \n",
    "def filter_invalid_ngrams(nested_list):\n",
    "    \"\"\"\n",
    "    Filters a nested list of n-gram tuples to remove invalid combinations.\n",
    "\n",
    "    An n-gram is considered invalid if it contains consecutive '<s>' or '</s>' tokens.\n",
    "    For example: ('<s>', '<s>'), ('<s>', '<s>', 'i'), ('sam', '</s>', '</s>')\n",
    "\n",
    "    Args:\n",
    "        nested_list: A list of lists, where each inner list contains tuples of strings.\n",
    "\n",
    "    Returns:\n",
    "        A new nested list with the invalid n-grams removed.\n",
    "    \"\"\"\n",
    "    filtered_nested_list = []\n",
    "    for inner_list in nested_list:\n",
    "        filtered_inner_list = []\n",
    "        for ngram_tuple in inner_list:\n",
    "            is_valid = True\n",
    "            # Check for consecutive <s> or </s> tokens\n",
    "            for i in range(len(ngram_tuple) - 1):\n",
    "                if (ngram_tuple[i] == '<s>' and ngram_tuple[i+1] == '<s>') or \\\n",
    "                   (ngram_tuple[i] == '</s>' and ngram_tuple[i+1] == '</s>'):\n",
    "                    is_valid = False\n",
    "                    break\n",
    "            \n",
    "            if is_valid:\n",
    "                filtered_inner_list.append(ngram_tuple)\n",
    "        \n",
    "        filtered_nested_list.append(filtered_inner_list)\n",
    "        \n",
    "    return filtered_nested_list\n",
    "\n",
    "def TextFile_Corpus(path = \"./corpus.txt\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads a text file and prepares a corpus by converting all text to lowercase \n",
    "    and stripping any leading or trailing whitespace from each line.\n",
    "\n",
    "    This function takes the path to a text file as input, reads its contents, \n",
    "    and processes each line to create a list of sentences. Each sentence is \n",
    "    converted to lowercase to ensure uniformity, making it easier to analyze \n",
    "    the text later on.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    path : str, optional\n",
    "        The file path to the text file containing the corpus. The default value \n",
    "        is \"./corpus.txt\", which means it will look for a file named 'corpus.txt' \n",
    "        in the current directory. You can specify a different path if your file \n",
    "        is located elsewhere.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    corpus : list of str\n",
    "        A list of strings, where each string represents a line from the text file. \n",
    "        Each line is processed to be in lowercase and stripped of any extra spaces. \n",
    "        This list serves as the basis for further text analysis or natural language \n",
    "        processing tasks.\n",
    "        \n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        corpus = [s.strip().lower() for s in f.readlines()]\n",
    "    return corpus\n",
    "\n",
    "\n",
    "corpus = TextFile_Corpus(path=\"./corpus.txt\")\n",
    "\n",
    "def get_Ngrams(train_data, n):\n",
    "    Ngrams = [[gram for gram in train_data[i] if len(gram) == n] for i in range(len(train_data)) ]\n",
    "    return Ngrams\n",
    "\n",
    "def prepare_corupus (corpus, n = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepares a text corpus for n-gram modeling by tokenizing the sentences, \n",
    "    generating n-grams, and organizing them into various structures.\n",
    "\n",
    "    This function takes a list of sentences (the corpus) and processes it to \n",
    "    create n-grams of specified lengths. It tokenizes the sentences, converts \n",
    "    them to lowercase, and generates padded n-grams. The output includes unique \n",
    "    n-grams, flattened n-grams, and the original n-grams in their nested form, \n",
    "    along with the training data and vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    corpus : list of str\n",
    "        A list of sentences (strings) that make up the text corpus. Each sentence \n",
    "        should be a coherent string of words. For example:\n",
    "        [\n",
    "            \"I am Sam\",\n",
    "            \"Sam I am\",\n",
    "            \"Sam I like\",\n",
    "            \"I do like Sam\",\n",
    "            \"do I like Sam\"\n",
    "        ]\n",
    "\n",
    "    n : int, optional\n",
    "        The maximum length of n-grams to generate. The default is 1, which means \n",
    "        the function will generate unigrams. If set to 2, it will generate \n",
    "        bigrams, and so on. This allows for flexibility in the type of n-grams \n",
    "        you want to analyze.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    unique_n_grams : dict\n",
    "        A dictionary where each key corresponds to the n-gram length (e.g., \n",
    "        '1-grams', '2-grams') and the value is a list of unique n-grams of that \n",
    "        length found in the corpus. This helps in understanding the distinct \n",
    "        phrases present in the text.\n",
    "\n",
    "    n_grams_flatten : dict\n",
    "        A dictionary similar to `unique_n_grams`, but the values are flattened \n",
    "        lists of n-grams. This means that all n-grams of a particular length \n",
    "        are combined into a single list, making it easier to analyze the \n",
    "        frequency of occurrences.\n",
    "\n",
    "    n_grams : dict\n",
    "        A dictionary containing the original nested structure of n-grams. Each \n",
    "        key corresponds to the n-gram length, and the value is a list of lists, \n",
    "        where each inner list contains the n-grams for a specific sentence in \n",
    "        the corpus. This structure retains the context of where each n-gram \n",
    "        originated.\n",
    "\n",
    "    train_data : list of list of tuples\n",
    "        A list of tuples representing the padded n-grams generated from the \n",
    "        tokenized sentences. This is the data that can be used to train an \n",
    "        n-gram language model.\n",
    "\n",
    "    vocab : list\n",
    "        A list of unique words (tokens) found in the corpus, which serves as the \n",
    "        vocabulary for the n-gram model. This is useful for understanding the \n",
    "        diversity of the language used in the corpus.\n",
    "\n",
    "    all_words : list\n",
    "        A list of all words (tokens) from the corpus, including duplicates. This \n",
    "        provides insight into the overall word usage in the text.\n",
    "\n",
    "    \"\"\"    \n",
    "    tokenized = [[w.lower() for w in word_tokenize(sent)] for sent in corpus]\n",
    "\n",
    "    train_data, vocab = padded_everygram_pipeline(n, tokenized)\n",
    "    all_words = list(vocab)\n",
    "    train_data = [list(g) for g in list(train_data)]\n",
    "    train_data = filter_invalid_ngrams(train_data)\n",
    "    \n",
    "    vocab = list(set(all_words))\n",
    "    \n",
    "    n_grams = {}\n",
    "    n_grams_flatten = {}\n",
    "    unique_n_grams = {}\n",
    "    \n",
    "    for j in range(n):\n",
    "        n_grams[f'{j+1}-grams'] = get_Ngrams(train_data, j+1)\n",
    "        \n",
    "    for k in range(n):\n",
    "        n_grams_flatten[f'{k+1}-grams'] = [item for sublist in n_grams[f'{k+1}-grams'] for item in sublist]\n",
    "\n",
    "    for t in range(n):\n",
    "        unique_n_grams[f'{t+1}-grams'] = list(set(n_grams_flatten[f'{t+1}-grams']))\n",
    "            \n",
    "    return unique_n_grams, n_grams_flatten, n_grams, train_data, vocab, all_words\n",
    "\n",
    "\n",
    "uninq_n_grams, n_grams_flatten, n_grams,train_data, vocab, all_words = prepare_corupus(corpus, n=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04dd49",
   "metadata": {},
   "source": [
    "## how to count grams or get n-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f5daf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram count for 'i': 5\n",
      "Bigram continuations after ('i',): [('am', 2), ('do', 1), ('like', 2)]\n",
      "C(i like) via counter: 2\n",
      "('<s>',) -> i: 2\n",
      "('<s>',) -> sam: 2\n",
      "('<s>',) -> do: 1\n",
      "('i',) -> am: 2\n",
      "('i',) -> like: 2\n",
      "('i',) -> do: 1\n",
      "('am',) -> sam: 1\n",
      "('am',) -> </s>: 1\n",
      "('sam',) -> </s>: 3\n",
      "('sam',) -> i: 2\n",
      "('like',) -> sam: 2\n",
      "('like',) -> </s>: 1\n",
      "('do',) -> like: 1\n",
      "('do',) -> i: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.lm import NgramCounter\n",
    "\n",
    "train_data_list = [list(s) for s in train_data] \n",
    "\n",
    "\n",
    "ng_counter = NgramCounter(train_data_list)\n",
    "\n",
    "\n",
    "print(\"Unigram count for 'i':\", ng_counter['i'])         # returns integer count of word on entire corpus \n",
    "print(\"Bigram continuations after ('i',):\", sorted(ng_counter[('i',)].items()))\n",
    "# get counts for full ngram 'i like' as indexing on a context then item:\n",
    "print(\"C(i like) via counter:\", ng_counter[['i']]['like'])   # equivalent: ng_counter[('i',)]['like']\n",
    "# Access all bigrams (order=2) as a ConditionalFreqDist:\n",
    "\n",
    "bigram_cfd = ng_counter[2]\n",
    "\n",
    "for context in bigram_cfd.conditions():    \n",
    "    for word in bigram_cfd[context]:\n",
    "        print(f\"{context} -> {word}: {bigram_cfd[context][word]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1cf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('<s>', 'i'): 0.4,\n",
       "  ('i', 'am'): 0.4,\n",
       "  ('am', 'sam'): 0.5,\n",
       "  ('sam', '</s>'): 0.6,\n",
       "  ('<s>', 'sam'): 0.4,\n",
       "  ('sam', 'i'): 0.4,\n",
       "  ('am', '</s>'): 0.5,\n",
       "  ('i', 'like'): 0.4,\n",
       "  ('like', '</s>'): 0.3333333333333333,\n",
       "  ('i', 'do'): 0.2,\n",
       "  ('do', 'like'): 0.5,\n",
       "  ('like', 'sam'): 0.6666666666666666,\n",
       "  ('<s>', 'do'): 0.2,\n",
       "  ('do', 'i'): 0.5}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEW FUNCTION:\n",
    "def get_all_ngram_probabilities(model, all_ngrams_flattened):\n",
    "    \"\"\"\n",
    "    Uses a trained MLE model to get probabilities for all n-grams.\n",
    "\n",
    "    Args:\n",
    "        model: The trained MLE language model.\n",
    "        all_ngrams_flattened: A dictionary of flattened lists of n-grams,\n",
    "                              keyed by their order (e.g., '1-grams', '2-grams').\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries containing all n-gram probabilities for each order.\n",
    "    \"\"\"\n",
    "    probabilities_list = [[] for _ in range(model.order)]\n",
    "    for k in range(1, model.order + 1):\n",
    "        prob_dict = {}\n",
    "        ngram_list = all_ngrams_flattened[f'{k}-grams']\n",
    "        for ngram in ngram_list:\n",
    "            if k == 1:\n",
    "                # Unigrams have no context\n",
    "                prob = model.score(ngram[0])\n",
    "            else:\n",
    "                # Higher-order n-grams have a context\n",
    "                context = ngram[:-1]\n",
    "                word = ngram[-1]\n",
    "                prob = model.score(word, context)\n",
    "            prob_dict[ngram] = prob\n",
    "        \n",
    "        probabilities_list[k-1].append(prob_dict)\n",
    "    return probabilities_list\n",
    "\n",
    "\n",
    "corpus = TextFile_Corpus(path=\"./corpus.txt\")\n",
    "uninq_n_grams, n_grams_flatten, n_grams, train_data, vocab, all_words = prepare_corupus(corpus, n=2)\n",
    "\n",
    "train_data_list = [list(s) for s in train_data]\n",
    "\n",
    "\n",
    "ng_counter = NgramCounter(train_data_list)\n",
    "\n",
    "\n",
    "# 4. Calculate probabilities using the new function\n",
    "n = 2 # Let's calculate for unigrams, bigrams, and trigrams\n",
    "lm = MLE(n)\n",
    "lm.fit(train_data, vocab)\n",
    "\n",
    "probabilities = get_all_ngram_probabilities(lm, n_grams_flatten)\n",
    "\n",
    "probabilities[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c5f286e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'<s>': 0.18518518518518517,\n",
       "  'i': 0.18518518518518517,\n",
       "  'am': 0.07407407407407407,\n",
       "  'sam': 0.18518518518518517,\n",
       "  '</s>': 0.18518518518518517,\n",
       "  'like': 0.1111111111111111,\n",
       "  'do': 0.07407407407407407},\n",
       " {('<s>', '<s>'): 0.08333333333333333,\n",
       "  ('<s>', 'i'): 0.08333333333333333,\n",
       "  ('<s>', 'am'): 0.08333333333333333,\n",
       "  ('<s>', 'sam'): 0.08333333333333333,\n",
       "  ('<s>', '</s>'): 0.08333333333333333,\n",
       "  ('<s>', 'like'): 0.08333333333333333,\n",
       "  ('<s>', 'do'): 0.08333333333333333,\n",
       "  ('i', '<s>'): 0.08333333333333333,\n",
       "  ('i', 'i'): 0.08333333333333333,\n",
       "  ('i', 'am'): 0.08333333333333333,\n",
       "  ('i', 'sam'): 0.08333333333333333,\n",
       "  ('i', '</s>'): 0.08333333333333333,\n",
       "  ('i', 'like'): 0.08333333333333333,\n",
       "  ('i', 'do'): 0.08333333333333333,\n",
       "  ('am', '<s>'): 0.1111111111111111,\n",
       "  ('am', 'i'): 0.1111111111111111,\n",
       "  ('am', 'am'): 0.1111111111111111,\n",
       "  ('am', 'sam'): 0.1111111111111111,\n",
       "  ('am', '</s>'): 0.1111111111111111,\n",
       "  ('am', 'like'): 0.1111111111111111,\n",
       "  ('am', 'do'): 0.1111111111111111,\n",
       "  ('sam', '<s>'): 0.08333333333333333,\n",
       "  ('sam', 'i'): 0.08333333333333333,\n",
       "  ('sam', 'am'): 0.08333333333333333,\n",
       "  ('sam', 'sam'): 0.08333333333333333,\n",
       "  ('sam', '</s>'): 0.08333333333333333,\n",
       "  ('sam', 'like'): 0.08333333333333333,\n",
       "  ('sam', 'do'): 0.08333333333333333,\n",
       "  ('</s>', '<s>'): 0.08333333333333333,\n",
       "  ('</s>', 'i'): 0.08333333333333333,\n",
       "  ('</s>', 'am'): 0.08333333333333333,\n",
       "  ('</s>', 'sam'): 0.08333333333333333,\n",
       "  ('</s>', '</s>'): 0.08333333333333333,\n",
       "  ('</s>', 'like'): 0.08333333333333333,\n",
       "  ('</s>', 'do'): 0.08333333333333333,\n",
       "  ('like', '<s>'): 0.1,\n",
       "  ('like', 'i'): 0.1,\n",
       "  ('like', 'am'): 0.1,\n",
       "  ('like', 'sam'): 0.1,\n",
       "  ('like', '</s>'): 0.1,\n",
       "  ('like', 'like'): 0.1,\n",
       "  ('like', 'do'): 0.1,\n",
       "  ('do', '<s>'): 0.1111111111111111,\n",
       "  ('do', 'i'): 0.1111111111111111,\n",
       "  ('do', 'am'): 0.1111111111111111,\n",
       "  ('do', 'sam'): 0.1111111111111111,\n",
       "  ('do', '</s>'): 0.1111111111111111,\n",
       "  ('do', 'like'): 0.1111111111111111,\n",
       "  ('do', 'do'): 0.1111111111111111}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from collections import defaultdict\n",
    "from itertools import permutations\n",
    "from task1 import prepare_corupus, TextFile_Corpus\n",
    "\n",
    "def addK_smoothing(ng_counter, max_order, k):\n",
    "    \"\"\"\n",
    "    Calculates the Add-k smoothed probabilities for n-grams up to max_order.\n",
    "\n",
    "    Args:\n",
    "        ng_counter: An NgramCounter object containing n-gram counts.\n",
    "        max_order: The maximum n-gram size to calculate probabilities for.\n",
    "        k: The smoothing parameter (e.g., k=1 for Laplace smoothing).\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries. The i-th dictionary contains the smoothed\n",
    "        probabilities for n-grams of size i+1.\n",
    "    \"\"\"\n",
    "    # Vocabulary size is needed for the denominator in the smoothing formula\n",
    "    # We get it from the unigram counts.\n",
    "    vocab_size = len(ng_counter[1].keys())\n",
    "    \n",
    "    probabilities_list = []\n",
    "    \n",
    "    # Iterate through each n-gram order from 1 to max_order\n",
    "    for order in range(1, max_order + 1):\n",
    "        prob_dict = {}\n",
    "        \n",
    "        if order == 1:\n",
    "            unigram_counts = ng_counter[1]\n",
    "            total_unigrams = unigram_counts.N()\n",
    "            \n",
    "            \n",
    "            if total_unigrams > 0:\n",
    "                for unigram, count in unigram_counts.items():\n",
    "                    prob = (count ) / total_unigrams\n",
    "                    prob_dict[unigram] = prob\n",
    "        else:\n",
    "\n",
    "            ngram_counts = ng_counter[order]\n",
    "            context_counts = ng_counter[order - 1]\n",
    "\n",
    "            all_contexts = list(context_counts.keys())\n",
    "            all_words = list(ng_counter[1].keys())\n",
    "            \n",
    "            for context in all_contexts:\n",
    "                context_count = context_counts.get(context, 0)\n",
    "                denominator = context_count + k * vocab_size\n",
    "                \n",
    "\n",
    "                for word in all_words:\n",
    "                    # Get the count of the full n-gram (context, word)\n",
    "                    # The .get() method returns 0 if the n-gram is unseen.\n",
    "                    ngram_count = ngram_counts[context].get(word, 0)\n",
    "                    \n",
    "                    prob = (ngram_count + k) / denominator\n",
    "                    ngram = (context,) + (word,)\n",
    "                    prob_dict[ngram] = prob\n",
    "        \n",
    "        probabilities_list.append(prob_dict)\n",
    "        \n",
    "    return probabilities_list\n",
    "\n",
    "corpus = TextFile_Corpus(path=\"./corpus.txt\")\n",
    "uninq_n_grams, n_grams_flatten, n_grams, train_data, vocab, all_words = prepare_corupus(corpus, n=2)\n",
    "\n",
    "\n",
    "train_data_list = [list(s) for s in train_data]\n",
    "\n",
    "ng_counter = NgramCounter(train_data_list)\n",
    "\n",
    "probabilities = addK_smoothing(ng_counter, 2, 1)\n",
    "\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbc1f0",
   "metadata": {},
   "source": [
    "# task 4 \n",
    "prepleixity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67b0224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from nltk.lm import Lidstone\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from itertools import chain\n",
    "from task1 import prepare_corupus, TextFile_Corpus, filter_invalid_ngrams\n",
    "\n",
    "\n",
    "# 2. Prepare the corpus and train the Lidstone model\n",
    "n = 2  # Bigram model\n",
    "gamma = 1  # Lidstone smoothing parameter (0 < gamma <= 1)\n",
    "# Note: Lidstone(gamma=1) is equivalent to Laplace (Add-1) smoothing.\n",
    "\n",
    "corpus = TextFile_Corpus(path=\"./corpus.txt\")\n",
    "\n",
    "tokenized_sentences = [[w.lower() for w in word_tokenize(sent)] for sent in corpus]\n",
    "\n",
    "\n",
    "train_data, vocab = padded_everygram_pipeline(n, tokenized_sentences)\n",
    "\n",
    "n = 2\n",
    "gamma= 1\n",
    "# Instantiate the Lidstone model\n",
    "lm = Lidstone(gamma, n)\n",
    "\n",
    "# Fit the model with the prepared data\n",
    "lm.fit(train_data, vocab)\n",
    "\n",
    "# 3. Prepare the test sentences for perplexity calculation\n",
    "test_sentence = \"I like Sam\"\n",
    "test_sentence_oov = \"I do not like Bob\"\n",
    "\n",
    "test_sentences = [test_sentence, test_sentence_oov]\n",
    "\n",
    "\n",
    "\n",
    "tokenized_test_sentences = [[w.lower() for w in word_tokenize(sent)] for sent in test_sentences]\n",
    "padded_test_data, _ = padded_everygram_pipeline(n, tokenized_test_sentences)\n",
    "test_data = [list(g) for g in list(padded_test_data)]\n",
    "test_data = filter_invalid_ngrams(test_data)\n",
    "\n",
    "\n",
    "# 4. Use the built-in .perplexity() method\n",
    "perplexity_score = lm.perplexity(list(test_data))\n",
    "\n",
    "print(perplexity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c3b8b5",
   "metadata": {},
   "source": [
    "The value you obtained is correct. A perplexity score of 8 is the exact result for that test case with a Lidstone model with γ=1 (Laplace smoothing) trained on your corpus.\n",
    "\n",
    "What Does a Perplexity of 8 Mean?\n",
    "A perplexity score of 8 can be interpreted as follows:\n",
    "\n",
    "On average, the model has the same level of uncertainty as if it were to choose uniformly from a set of 8 equally likely words at each point in the test sentence.\n",
    "\n",
    "Since your vocabulary has 7 words (∣V∣=7), a perplexity of 8 is very high. It means the model is almost as \"perplexed\" as it would be if it had to choose from your entire vocabulary at random for every single word.\n",
    "\n",
    "This high perplexity is expected because your test data includes an out-of-vocabulary word (Bob). Even though the smoothing gives the unseen bigrams a small non-zero probability, the model is still highly \"surprised\" by them. This shows that your model, while working correctly, is not a good fit for a test set that contains words it has never seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74832d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
