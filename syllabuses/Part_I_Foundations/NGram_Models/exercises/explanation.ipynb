{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060b3179",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #ece9ff 0%, #d0d0ffff 55%); color: #0f172a; border-radius: 12px; padding: 20px; box-shadow: 0 6px 20px rgba(15,23,42,0.08); font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial; line-height:1.5;\">\n",
    "\n",
    "<h2 style=\"margin-top:0;\">ðŸŒŸ Maximum-Likelihood Estimation (MLE) â€” Bigram (n = 2)</h2>\n",
    "\n",
    "<p>\n",
    "This single Markdown/HTML block explains how the **MLE bigram model** works with NLTK's\n",
    "<code>padded_everygram_pipeline</code>. It is written to be **pasted into a Markdown cell** in a Jupyter\n",
    "notebook. The design uses a gentle, lighter purple/blue background for readability while keeping\n",
    "contrast accessible.\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "<h2>1. Short conceptual summary</h2>\n",
    "\n",
    "<ul>\n",
    "  <li>**Goal:** estimate conditional probabilities of the form \\(P(w_t \\mid w_{t-1})\\) using counts from a corpus.</li>\n",
    "  <li>**Estimator (MLE):**\n",
    "  <div style=\"background: rgba(255,255,255,0.84); padding:10px; border-radius:8px; display:inline-block;\">\n",
    "\n",
    "$$\n",
    "at{P}(w_t \\mid w_{t-1}) = \\frac{C(w_{t-1}, w_t)}{C(w_{t-1})}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "  </li>\n",
    "  <li>**Padding:** sentences are padded with <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> so we can estimate start and end probabilities.</li>\n",
    "  <li>**No smoothing in this cell:** unseen bigrams get probability 0 (we will add smoothing later if requested).</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h2>2. How <code>padded_everygram_pipeline(n, tokenized)</code> prepares training data</h2>\n",
    "\n",
    "<ul>\n",
    "  <li>**Input:** <code>tokenized</code> is a list of token lists. Example: <code>[[\"i\",\"am\",\"sam\"], [...], ...]</code>.</li>\n",
    "  <li>**n:** order of the model; here <code>n = 2</code> (bigrams).</li>\n",
    "  <li>**Returns:**\n",
    "    <ul>\n",
    "      <li><code>train_data</code>: generator yielding everygrams (all 1..n-grams) for each padded sentence.</li>\n",
    "      <li><code>vocab</code>: vocabulary iterable used for training.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Why <code>everygram</code> and padding?</strong></p>\n",
    "<ul>\n",
    "  <li><code>everygram</code> yields *both* unigrams and bigrams for each sentence (helpful for some training APIs and introspection).</li>\n",
    "  <li>Padding with <code>&lt;s&gt;</code> tokens (one start token for bigrams) and <code>&lt;/s&gt;</code> allows modeling first-word probabilities.</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h2>3. Expected structure (<code>tmp</code> after <code>list([list(g) for g in train_data])</code>)</h2>\n",
    "\n",
    "<p>For clarity, here is the *expected* output structure (one inner list per sentence) for your corpus (lowercased tokens):</p>\n",
    "\n",
    "<p><strong>Corpus (lowercased token lists):</strong><br>\n",
    "<code>[[\"i\",\"am\",\"sam\"], [\"sam\",\"i\",\"am\"], [\"sam\",\"i\",\"like\"], [\"i\",\"do\",\"like\",\"sam\"], [\"do\",\"i\",\"like\",\"sam\"]]</code></p>\n",
    "\n",
    "<p><strong><code>tmp</code> (per-sentence everygrams â€” unigrams + bigrams if n = 2):</strong></p>\n",
    "\n",
    "<ul>\n",
    "  <li>Sentence 1 (<code>\"i am sam\"</code>) padded <code>['&lt;s&gt;', 'i', 'am', 'sam', '&lt;/s&gt;']</code> â†’ everygrams:\n",
    "    <ul>\n",
    "      <li>1-grams: <code>('&lt;s&gt;',) ('i',) ('am',) ('sam',) ('&lt;/s&gt;',)</code></li>\n",
    "      <li>2-grams: <code>('&lt;s&gt;','i') ('i','am') ('am','sam') ('sam','&lt;/s&gt;')</code></li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Sentence 2 (<code>\"sam i am\"</code>) padded <code>['&lt;s&gt;', 'sam', 'i', 'am', '&lt;/s&gt;']</code> â†’ everygrams similarly.</li>\n",
    "  <li>Sentence 3 (<code>\"sam i like\"</code>) padded <code>['&lt;s&gt;', 'sam', 'i', 'like', '&lt;/s&gt;']</code>.</li>\n",
    "  <li>Sentence 4 (<code>\"i do like sam\"</code>) padded <code>['&lt;s&gt;', 'i', 'do', 'like', 'sam', '&lt;/s&gt;']</code>.</li>\n",
    "  <li>Sentence 5 (<code>\"do i like sam\"</code>) padded <code>['&lt;s&gt;', 'do', 'i', 'like', 'sam', '&lt;/s&gt;']</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Then you split <code>tmp</code> into unigrams and bigrams using list comprehensions:</p>\n",
    "\n",
    "<pre><code style=\"background-color:#f4f4f4; padding:8px; border-radius:5px;\">bigrams_sentences = [[gram for gram in tmp[i] if len(gram) == 2] for i in range(len(tmp))]\n",
    "unigrams_sentences = [[gram for gram in tmp[i] if len(gram) == 1] for i in range(len(tmp))]</code></pre>\n",
    "\n",
    "<p>Each <code>bigrams_sentences[i]</code> is a list of bigram tuples for sentence <code>i</code>.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2>4. Concrete counting example â€” computing \\(P(\\text{like}\\mid i)\\)</h2>\n",
    "\n",
    "<p><strong>Step 1: extract and count all bigrams from the padded corpus.</strong><br>\n",
    "From the five sentences above, the relevant bigrams involving <code>i</code> are:</p>\n",
    "\n",
    "<ul>\n",
    "  <li><code>('i', 'am')</code> Ã— 2</li>\n",
    "  <li><code>('i', 'like')</code> Ã— 2</li>\n",
    "  <li><code>('i', 'do')</code> Ã— 1</li>\n",
    "</ul>\n",
    "\n",
    "<p>So the **counts** (written unambiguously) are:</p>\n",
    "\n",
    "<div style=\"background: rgba(255,255,255,0.84); padding:10px; border-radius:8px; display:inline-block;\">\n",
    "\n",
    "$$\n",
    "C((i,like)) = 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "C(i) = \\sum_{w} C((i,w)) = 2+2+1 = 5\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "<p><strong>Step 2: apply the MLE formula</strong></p>\n",
    "\n",
    "$$\n",
    "\\hat{P}(\\text{like}\\mid i)\n",
    "= \\frac{C((\\text{i},\\;\\text{like}))}{C(\\text{i})}\n",
    "= \\frac{2}{5}\n",
    "= 0.4\n",
    "$$\n",
    "\n",
    "<p>This is exactly what <code>model.score('like', ['i'])</code> will return after training <code>MLE(order=2)</code> with <code>train_data</code> and <code>vocab</code>.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2>5. Minimal code snippet (run in a code cell after this Markdown)</h2>\n",
    "\n",
    "<pre><code style=\"background-color:#f4f4f4; padding:8px; border-radius:5px;\">from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = [\n",
    "    \"I am Sam\",\n",
    "    \"Sam I am\",\n",
    "    \"Sam I like\",\n",
    "    \"I do like Sam\",\n",
    "    \"do I like Sam\",\n",
    "]\n",
    "\n",
    "# tokenization (lowercase)\n",
    "tokenized = [[w.lower() for w in word_tokenize(sent)] for sent in corpus]\n",
    "\n",
    "n = 2\n",
    "train_data, vocab = padded_everygram_pipeline(n, tokenized)\n",
    "# inspect prepared everygrams\n",
    "tmp = list([list(g) for g in train_data])\n",
    "\n",
    "# split into unigrams and bigrams\n",
    "bigrams_sentences = [[gram for gram in tmp[i] if len(gram) == 2] for i in range(len(tmp))]\n",
    "unigrams_sentences = [[gram for gram in tmp[i] if len(gram) == 1] for i in range(len(tmp))]\n",
    "\n",
    "print(\"bigrams_sentences:\\n\", bigrams_sentences)\n",
    "print(\"unigrams_sentences:\\n\", unigrams_sentences)\n",
    "\n",
    "# train MLE model\n",
    "model = MLE(order=n)\n",
    "model.fit(train_data, vocab)\n",
    "print(\"P(like|i) =\", model.score('like', ['i']))</code></pre>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2 style=\"margin-top:0;\">ðŸŒŸ Extending the MLE Bigram Model â€” Next-word distribution, sentence probability, and notes</h2>\n",
    "\n",
    "---\n",
    "\n",
    "<h2>6. Next-word distribution (Probability Mass Function / PMF)</h2>\n",
    "\n",
    "<p>Once the MLE bigram model is trained, you can compute the **distribution of possible next words given a context**.</p>\n",
    "\n",
    "<p>Python snippet (explained):</p>\n",
    "\n",
    "<pre><code style=\"background-color:#f4f4f4; padding:8px; border-radius:5px;\">\n",
    "# Vocabulary tokens (includes <UNK> for unseen words)\n",
    "vocab_tokens = list(model.vocab)\n",
    "\n",
    "# Compute raw probabilities for each word given the context ['i']\n",
    "raw = {w: model.score(w, ['i']) for w in vocab_tokens}\n",
    "\n",
    "# Normalize to get a proper probability distribution (PMF)\n",
    "total = sum(raw.values())\n",
    "dist = {w: (cnt/total if total>0 else 0.0) for w, cnt in raw.items()}\n",
    "\n",
    "# Only display non-zero probabilities\n",
    "print(\"Next-word distribution given context ('i',):\")\n",
    "print({k: v for k, v in dist.items() if v>0})\n",
    "</code></pre>\n",
    "\n",
    "<p><strong>Explanation:</strong></p>\n",
    "<ul>\n",
    "<li><code>model.score(word, context)</code> returns the MLE probability of <code>word</code> given <code>context</code>.</li>\n",
    "<li>We iterate over the vocabulary (including <code>&lt;UNK&gt;</code>) to get all potential next words.</li>\n",
    "<li>Normalization ensures the probabilities sum to 1 â€” now you have a proper **Probability Mass Function (PMF)** over the next word.</li>\n",
    "<li>Non-zero filtering is optional but helps readability.</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h2>7. Sentence probability</h2>\n",
    "\n",
    "<p>MLE allows us to compute the probability of an entire sentence by multiplying conditional probabilities:</p>\n",
    "\n",
    "<pre><code style=\"background-color:#f4f4f4; padding:8px; border-radius:5px;\">\n",
    "def sentence_prob(sentence, model, n):\n",
    "    # Tokenize and lowercase\n",
    "    toks = [w.lower() for w in word_tokenize(sentence)]\n",
    "    # Pad with <s> (n-1 times) and </s> for start/end\n",
    "    padded = ['<s>']*(n-1) + toks + ['</s>']\n",
    "    \n",
    "    prob = 1.0\n",
    "    for i in range(n-1, len(padded)):\n",
    "        context = padded[i-(n-1):i] if n>1 else []\n",
    "        w = padded[i]\n",
    "        p = model.score(w, context)\n",
    "        prob *= p\n",
    "        if prob == 0.0:\n",
    "            return 0.0  # MLE without smoothing can produce 0 probability for unseen n-grams\n",
    "    return prob\n",
    "\n",
    "print(\"P('I like Sam') =\", sentence_prob(\"I like Sam\", model, n))\n",
    "</code></pre>\n",
    "\n",
    "<p><strong>Explanation:</strong></p>\n",
    "<ul>\n",
    "<li>Each word probability is multiplied sequentially using the preceding <code>n-1</code> words as context.</li>\n",
    "<li>If the sentence contains an **unseen bigram**, the probability becomes 0. This is expected without smoothing.</li>\n",
    "<li>Padding with <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> ensures start-of-sentence and end-of-sentence probabilities are included.</li>\n",
    "</ul>\n",
    "\n",
    "## 8. Cross-Entropy\n",
    "\n",
    "The **cross-entropy** between the empirical distribution \\(p\\) and our model distribution \\(\\hat{p}\\) is defined as:\n",
    "\n",
    "$$\n",
    "H(p,\\hat{p}) = - \\sum_{x} p(x)\\,\\log \\hat{p}(x)\n",
    "$$\n",
    "\n",
    "For a test sequence \\(w_1^n = w_1, w_2, \\dots, w_n\\):\n",
    "\n",
    "$$\n",
    "H(p,\\hat{p}) = -\\frac{1}{n}\\sum_{t=1}^{n} \\log \\hat{P}(w_t \\mid w_{t-1})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Perplexity\n",
    "\n",
    "The **perplexity** of the model on a test sequence is:\n",
    "\n",
    "$$\n",
    "\\text{PP}(w_1^n)\n",
    "= \\exp\\!\\Big(-\\frac{1}{n}\\sum_{t=1}^{n}\\log \\hat{P}(w_t \\mid w_{t-1})\\Big)\n",
    "$$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$$\n",
    "\\text{PP}(w_1^n) = 2^{H(p,\\hat{p})}\n",
    "$$\n",
    "\n",
    "So lower perplexity means the model assigns higher probability to the observed sequence.\n",
    "\n",
    "---\n",
    "\n",
    "<h2>10. Key practical notes and caveats</h2>\n",
    "\n",
    "<ul>\n",
    "<li>Always **tokenize and lowercase** consistently between training and evaluation.</li>\n",
    "<li>MLE alone is **exact but brittle** â€” unseen n-grams get probability 0.</li>\n",
    "<li>Including <code>&lt;UNK&gt;</code> allows for unknown words but may slightly skew probability mass.</li>\n",
    "<li>Generators (like <code>train_data</code>) can be consumed only once. Convert to list if needed multiple times.</li>\n",
    "<li>Next steps: implement **smoothing** (Laplace, Kneser-Ney, etc.) to avoid zero probabilities and stabilize perplexity.</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>âœ… How to use in your notebook</h3>\n",
    "<ol>\n",
    "  <li>Paste this Markdown/HTML block into a single cell to explain the bigram MLE pipeline.</li>\n",
    "  <li>Run separate Python code cells for training the model, computing sentence probabilities, and next-word distributions.</li>\n",
    "  <li>Use it as a foundation before introducing smoothing or trigram/higher-order models.</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b49590f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
